---
title: "STATISTICAL LEARNING"
subtitle: "Homework 1"
author: "Leoni Paolo, Morosini Francesco, Protani Andrea, Sottile Alessandro, Tarantino Ramona"
abstract: |
    This work focuses on implementing splines using the power truncated basis method and applying them to make predictions on WMAP data. WMAP data provides a snapshot of our universe during its infancy, approximately 379,000 years after the Big Bang. Splines are an interpolation technique that divides the domain into smaller segments and relies on local polynomial functions to approximate the data. The power truncated basis method constructs the basis functions using powers of polynomial terms, with some terms removed to ensure good damping properties. The main objective of this homework was to implement splines using the power truncated basis method from scratch. This involved understanding the concept of splines, selecting appropriate basis functions, and implementing the algorithm to calculate spline coefficients.
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: hide
    theme: bootstrap
    highlight: tango
---

```{css style settings, echo = FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

library(caret)
library(doParallel)
library(glmnet)
library(tidyr)
library(gridExtra)

```

## Be linear in transformed feature space & orthogonal series expansion  {.tabset-fade}

This idea use truncated power functions $G_{d,q}(x)$ as the transformed features and $f(\cdot)$ is the spline function with knots that can be expressed as a linear combination of $G_{d,q}$:

$f(x) = \sum_{j} \beta_j \cdot g_j(x)$, allows to express the complex spline function in terms of a set of coefficients $\beta_j$ and transformed features $g_j(x)$ to semplify representation, analysis and modelling.

Both orthogonal series expansion and spline regression are used to model nonlinear relationships with a sum of more simple function to represent a more
complicated function.

In orthogonal series expansion, the basis functions used to model the data are orthogonal functions, such as a sum of sine and cosine functions.

Spline regressions use a series of knots and non-linear curves, composed of segments of polynomials that join continuously, to fit the data. This enable splines to focusing on parts of the data that require more complexity.

## Truncated power basis {.tabset-fade}

### Our implementation

To create the design matrix based on the **truncated power basis** $G_{d,q}$ we defined the following function:

```{r}

power_basis <- function(x, d, q_1, q_2=0, point=1, start=0, stop=1){
  
  # x      <--  vector of observation
  # d      <--  degree of the polynomial
  # q_1    <--  number of knots before 'point'
  # q_2    <--  number of knots after 'point'
  # point  <--  point which divide 'start-stop' in two chuncks
  # start  <--  min value of the x support
  # stop   <--  max value of the x support
  
  # calculate the coordinate of the knots
  if (q_2==0){
    knots <- seq(from=start, to=stop, length=q_1+2)[-c(1,q_1+2)]
  }
  else{
    knots <- c(seq(from=start, to=point, length=q_1+2)[-c(1)], seq(from=point, to=stop, length=q_2+2)[-c(1,q_2+2)]) 
  }
  
  # compute the coefficients of the polynomial
  pol_1 <- poly(x, degree=d, simple=T, raw=T)
  
  # subtracts the knots values from each point
  pol_2 <- sapply(x, function(a) (a - knots))
  
  # set to 0 the values that were <0
  pol_2[pol_2 < 0] <- 0
  
  # raise all points to the power d
  pol_2 <- pol_2^d
  
  # combine it all into an array
  X <- cbind(rep(1, length(x)), pol_1, t(pol_2))
  
  return(X)
  
}

```

We can now plot some of the basis generated for different combinations of $d$ and $q$.

In particular for $d \in \{1, 3 \}$ and $q \in \{3, 10 \}$

```{r fig.height=10, fig.width=12, message=FALSE, warning=FALSE}

support <- seq(0, 1, 0.001)


### ggplot 1 ###

d1 <- 1
q1 <- 3
X1 <- cbind(support, power_basis(x=support, d=d1, q_1=q1))

df1 <- data.frame(X1)
colnames(df1) <- c('x', 'x^0', 'x^1', '(x-q1)^1', '(x-q2)^1', '(x-q3)^1')
df1 <- df1[, -2]

df_1 <- gather(df1, key="Basis", value="value", -x)
df_1$Basis <- factor(df_1$Basis, levels=c('x^1', '(x-q1)^1', '(x-q2)^1', '(x-q3)^1'))


p1 <- ggplot(df_1, aes(x=x, y=value, color=Basis)) + 
  geom_line(size=1.2) + 
  labs(x='x', y='f(x)', title = paste("Truncated power functions with: ", "d =", d1, " |  q =", q1)) + 
  theme_bw() + 
  scale_color_brewer(palette="Pastel1")


### ggplot 2 ###

d2 <- 1
q2 <- 10
X2 <- cbind(support, power_basis(x=support, d=d2, q_1=q2))

df2 <- data.frame(X2)
colnames(df2) <- c('x', 'x^0', 'x^1', '(x-q1)^1', '(x-q2)^1', '(x-q3)^1', '(x-q4)^1', '(x-q5)^1',
                   '(x-q6)^1', '(x-q7)^1', '(x-q8)^1', '(x-q9)^1', '(x-q10)^1')
df2 <- df2[, c('x', 'x^1', '(x-q2)^1', '(x-q4)^1', '(x-q6)^1', '(x-q8)^1', '(x-q10)^1')]

df_2 <- gather(df2, key="Basis", value="value", -x)
df_2$Basis <- factor(df_2$Basis, levels=c('x^1', '(x-q2)^1', '(x-q4)^1', '(x-q6)^1', '(x-q8)^1', '(x-q10)^1'))

p2 <- ggplot(df_2, aes(x=x, y=value, color=Basis)) + 
  geom_line(size=1.2) + 
  labs(x='x', y='f(x)', title = paste("Truncated power functions with: ", "d =", d2, " |  q =", q2)) + 
  theme_bw() + 
  scale_color_brewer(palette="Pastel1")


### ggplot 3 ###

d3 <- 3
q3 <- 3
X3 <- cbind(support, power_basis(x=support, d=d3, q_1=q3))

df3 <- data.frame(X3)
colnames(df3) <- c('x', 'x^0', 'x^1', 'x^2', 'x^3', '(x-q1)^3', '(x-q2)^3', '(x-q3)^3')
df3 <- df3[, -2]

df_3 <- gather(df3, key="Basis", value="value", -x)
df_3$Basis <- factor(df_3$Basis, levels=c('x^1', 'x^2', 'x^3', '(x-q1)^3', '(x-q2)^3', '(x-q3)^3'))

p3 <- ggplot(df_3, aes(x=x, y=value, color=Basis)) + 
  geom_line(size=1.2) + 
  labs(x='x', y='f(x)', title = paste("Truncated power functions with: ", "d =", d3, " |  q =", q3)) + 
  theme_bw() + 
  scale_color_brewer(palette="Pastel1")


### ggplot 4 ###

d4 <- 3
q4 <- 10
X4 <- cbind(support, power_basis(x=support, d=d4, q_1=q4))

df4 <- data.frame(X4)
colnames(df4) <- c('x', 'x^0', 'x^1', 'x^2', 'x^3', '(x-q1)^3', '(x-q2)^3', '(x-q3)^3', '(x-q4)^3',
                   '(x-q5)^3', '(x-q6)^3', '(x-q7)^3', '(x-q8)^3', '(x-q9)^3', '(x-q10)^3')
df4 <- df4[, c('x', 'x^1', 'x^2', 'x^3', '(x-q2)^3', '(x-q4)^3', '(x-q6)^3', '(x-q8)^3', '(x-q10)^3')]

df_4 <- gather(df4, key="Basis", value="value", -x)
df_4$Basis <- factor(df_4$Basis, levels=c('x^1', 'x^2', 'x^3', '(x-q2)^3', '(x-q4)^3', '(x-q6)^3', '(x-q8)^3', '(x-q10)^3'))

p4 <- ggplot(df_4, aes(x=x, y=value, color=Basis)) + 
  geom_line(size=1.2) + 
  labs(x='x', y='f(x)', title = paste("Truncated power functions with: ", "d =", d4, " |  q =", q4)) + 
  theme_bw() + 
  scale_color_brewer(palette="Pastel1")


### plot ###

grid.arrange(p1, p2, p3, p4, ncol = 2)


```

### ChatGPT implementation

Below it's reported the conversation had with ChatGPT in order to make it create the **Truncated power basis** function correctly.

<details>
  <summary>Click to expand</summary>
  <span style="font-size: 5;"> 

> US:
> 
> Develop in R your own implementation of the truncated power basis Gd,q , and then plot a few elements with d ∈ {1, 3} and q ∈ {3, 10} equispaced knots in the open interval (0, 1).
 

> ChatGPT:
> 
> To implement the truncated power basis Gd,q, we can use the following R function:
> ```
> G <- function(x, d, q) {
  basis <- matrix(0, nrow = length(x), ncol = d*q+1)
  basis[,1] <- 1
  for (j in 1:d) {
    for (k in 1:q) {
      basis[,j*q+k] <- (x^(j-1))*((x >= ((k-1)/q) & (x < k/q))) 
    }
  }
  return(basis)
}
> ```
>The function takes three arguments: x, the vector of points where we want to evaluate the basis; d, the degree of the polynomial basis; and q, the number of equispaced knots in the open interval (0,1). The output of the function is a matrix with length(x) rows and d*q+1 columns, where the first column is a vector of ones (corresponding to the constant term of the polynomial basis) and the remaining columns contain the basis functions.
> 
> To plot some elements of the basis, we can use the following R code: 
> ```
> # Define values of d and q
d_values <- c(1, 3)
q_values <- c(3, 10)
# Define values of x where to evaluate the basis
x <- seq(0, 1, length.out = 1000)
# Plot basis functions for each combination of d and q
par(mfrow = c(length(d_values), length(q_values)))
for (i in seq_along(d_values)) {
  for (j in seq_along(q_values)) {
    d <- d_values[i]
    q <- q_values[j]
    basis <- G(x, d, q)
    plot(x, basis[,1], type = "l", col = "black", xlab = "x", ylab = "G(x)", main = paste0("d = ", d, ", q = ", q))
    lines(x, basis[,2], type = "l", col = "red")
    lines(x, basis[,3], type = "l", col = "green")
    lines(x, basis[,4], type = "l", col = "blue")
  }
}
> ```
> This code will create a plot with four panels, each corresponding to a combination of d and q. In each panel, we plot the first four basis functions (excluding the constant term) as black, red, green, and blue lines.


> US:
> 
> there is an error in the function G:
>
> Error in `[<-`(`*tmp*`, , j * q + k, value = (x^(j - 1)) * ((x >= ((k -  : subscript out of bounds


> ChatGPT:
> 
> The error message "subscript out of bounds" indicates that we are trying to access an element of the basis matrix that does not exist. This can happen if the values of d and q are such that we are trying to access a column that is beyond the last column of the matrix.
> 
> One possible reason for this error is  that the formula for the number of columns in the basis matrix, d*q+1, is incorrect. If d and q are not chosen carefully, this formula may produce a number of columns that exceeds the size of the basis matrix.
> 
> To fix this error, we can modify the G function as follows:
> ```
> G <- function(x, d, q) {
  basis <- matrix(0, nrow = length(x), ncol = d*q+1)
  basis[,1] <- 1
  for (j in 1:d) {
    for (k in 1:q) {
      col_index <- (j-1)*q+k+1
      basis[,col_index] <- (x^(j-1))*((x >= ((k-1)/q) & (x < k/q))) 
    }
  }
  return(basis)
}
> ```
> In this version of the function, we use the expression (j-1)q+k+1 to calculate the index of the column where we want to store the basis function. This ensures that we always have dq+1 columns in the basis matrix, regardless of the values of d and q.
> 
> With this modification, the G function should be able to handle any combination of d and q without producing a "subscript out of bounds" error.

> US:
> 
> the function doesn't do what is supposed to, I want in output a matrix such that:
> 
> column 1 -> x^1
> 
> column 2 -> x^2
> 
> ...
> 
> column d -> x^d
> 
> column d+1 -> (x-knot1)^d
> 
> column d+2 -> (x-knot2)^d
> 
> ... 
> 
> column d+q -> (x-knotq)^d


> ChatGPT:
> 
> I apologize for the mistake in my previous implementation of the G function. Here is a corrected version of the function that should produce the output you described:
> ```
> G <- function(x, d, q) {
  knots <- seq(1/(2*q), 1 - 1/(2*q), length.out = q)
  basis <- matrix(0, nrow = length(x), ncol = d*q+d)
  basis[,1:d] <- x^(1:d)
  for (j in 1:d) {
    for (k in 1:q) {
      basis[,d+(j-1)*q+k] <- (x-knots[k])^j * ((x >= knots[k] & x < knots[k]+1/q) | (j == 1 & x == 1))
    }
  }
  return(basis)
}
> ```
> The function takes three arguments: x, the vector of points where we want to evaluate the basis; d, the degree of the polynomial basis; and q, the number of equispaced knots in the open interval (0,1). The output of the function is a matrix with length(x) rows and dq+d columns, where the first d columns are the monomial terms x^1, x^2, ..., x^d, and the remaining dq columns contain the basis functions of the form (x-knot)^j, for j = 1, 2, ..., d and each of the q knots.
> 
> The knots are defined as the midpoints of q equispaced intervals in the open interval (0,1). The basis functions are defined as piecewise polynomials that are nonzero on each of the intervals between consecutive knots, and zero elsewhere. The exception is the last interval, which includes the point x=1, where we set the basis function to be nonzero if j=1.
> 
> You can use this function to evaluate the basis functions for different values of d and q. For example, to plot the first 10 basis functions for d=3 and q=4, you can use the following code:
> ```
> x <- seq(0, 1, length.out = 1000)
basis <- G(x, 3, 4)
plot(x, basis[,1], type = "l", col = "black", xlab = "x", ylab = "G(x)")
for (i in 2:10) {
  lines(x, basis[,i], type = "l", col = i)
}
> ```
> This code will create a plot with 10 curves, each corresponding to a basis function of the truncated power basis with d=3 and q=4. The first curve is the monomial term x^1, and the remaining curves are the basis functions of the form (x-knot)^j for j=1, 2, 3 and each of the 4 knots.


> US:
> 
> the following code --> basis[,1:d] <- x^(1:d) doesn't work well, it's not filling the columns in the correct way, the first column must be x^1, the second column x^2 and so on


> ChatGPT:
> 
> You're correct, I apologize for the mistake. To assign the first d columns of the basis matrix to the monomial terms x^1, x^2, ..., x^d, we can modify the line that you mentioned to:
> ```
> basis[, 1:d] <- matrix(x, ncol = d)^rep(1:d, each = length(x))
> ```
> This code creates a matrix of the same dimensions as basis[, 1:d], where each column contains the vector x raised to a power from 1 to d. The rep function is used to repeat each element of 1:d length(x) times, so that the resulting matrix has d columns and length(x) rows.
> 
> Here's the complete corrected implementation of the G function:
> ```
> G <- function(x, d, q) {
  knots <- seq(1/(2*q), 1 - 1/(2*q), length.out = q)
  basis <- matrix(0, nrow = length(x), ncol = d*q+d)
  basis[, 1:d] <- matrix(x, ncol = d)^rep(1:d, each = length(x))
  for (j in 1:d) {
    for (k in 1:q) {
      basis[, d+(j-1)*q+k] <- (x-knots[k])^j * ((x >= knots[k] & x < knots[k]+1/q) | (j == 1 & x == 1))
    }
  }
  return(basis)
}
> ```
> With this modification, the first d columns of the basis matrix will contain the monomial terms x^1, x^2, ..., x^d, and the remaining columns will contain the basis functions of the form (x-knot)^j, for j = 1, 2, ..., d and each of the q knots.


> US:
>
> whit this code:
> 
> basis[, 1:d] <- matrix(x, ncol = d)^rep(1:d, each = length(x))
> 
> I get this error:
> 
> Error: dims [product 6] do not match the length of object [10]
> 
> In addition: Warning messages:
> 
> 1: In matrix(x, ncol = d) : data length [5] is not a sub-multiple or multiple of the number of rows [3]
> 
> 2: In matrix(x, ncol = d)^rep(1:d, each = length(x)) : longer object length is not a multiple of shorter object length


> ChatGPT:
> 
> I see, that error is because the length of x is not a multiple of d. To fix this, we can change the way we create the matrix of powers using the outer function instead of matrix and rep. Here's the modified line:
> ```
> basis[, 1:d] <- outer(x, 1:d, FUN = "^")
> ```
> This creates a matrix where each column is the vector x raised to a power from 1 to d.
> 
> Here's the complete corrected implementation of the G function:
> ```
> G <- function(x, d, q) {
  knots <- seq(1/(2*q), 1 - 1/(2*q), length.out = q)
  basis <- matrix(0, nrow = length(x), ncol = d*q+d)
  basis[, 1:d] <- outer(x, 1:d, FUN = "^")
  for (j in 1:d) {
    for (k in 1:q) {
      basis[, d+(j-1)*q+k] <- (x-knots[k])^j * ((x >= knots[k] & x < knots[k]+1/q) | (j == 1 & x == 1))
    }
  }
  return(basis)
}
> ```
> With this modification, the first d columns of the basis matrix will contain the monomial terms x^1, x^2, ..., x^d, and the remaining columns will contain the basis functions of the form (x-knot)^j, for j = 1, 2, ..., d and each of the q knots.

> US:
>
> ok this code basis[, 1:d] <- outer(x, 1:d, FUN = "^")  works, now you have to fill the other q columns in this way:
> 
> col d+1 -> (x-knot1)^d
> 
> col d+2 -> (x-knot2)^d
> 
> ...
> 
> col d+q -> (x-knotq)^d


> ChatGPT:
>
> Sure, here's the modified version of the G function that fills in the remaining columns of the basis matrix:
> ```
> G <- function(x, d, q) {
  knots <- seq(1/(2*q), 1 - 1/(2*q), length.out = q)
  basis <- matrix(0, nrow = length(x), ncol = d*q+d)
  basis[, 1:d] <- outer(x, 1:d, FUN = "^")
  for (j in 1:d) {
    for (k in 1:q) {
      basis[, d+(j-1)*q+k] <- (x-knots[k])^j * ((x >= knots[k] & x < knots[k]+1/q) | (j == 1 & x == 1))
    }
  }
  return(basis)
}
> ```
> Now, the for loop iterates over each knot k and fills in the d+1, d+2, ..., d+q columns with the values (x-knot)^d. Note that the logical expression (x >= knots[k] & x < knots[k]+1/q) | (j == 1 & x == 1) is used to ensure that the basis functions are zero outside of the interval between consecutive knots, except for the special case of j=1 and x=1.


> US:
>
> this code to create the knots do not works as it should:
> 
> knots <- seq(1/(2*q), 1 - 1/(2*q), length.out = q)
> 
> i want q equispaced knots that divide the support in q+2 equal wide intervals


> ChatGPT:
>
> I apologize for the confusion. You're right that the original code for the knots doesn't produce equispaced knots. Here's the corrected code that should produce q equispaced knots that divide the support in q+2 equal wide intervals:
> ```
> knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)][-1]
> ```
> This creates a sequence of q+2 equally spaced points on the interval [0, 1], removes the first and last points (which are outside the support), and then removes the first remaining point (which is the left endpoint of the first interval) to obtain q equispaced knots that divide the support into q+2 equal intervals.
> 
> So the modified G function with this corrected code for the knots is:
> ```
> G <- function(x, d, q) {
  knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)][-1]
  basis <- matrix(0, nrow = length(x), ncol = d*q+d)
  basis[, 1:d] <- outer(x, 1:d, FUN = "^")
  for (j in 1:d) {
    for (k in 1:q) {
      basis[, d+(j-1)*q+k] <- (x-knots[k])^j * ((x >= knots[k] & x < knots[k+1]) | (j == 1 & x == 1))
    }
  }
  return(basis)
}
> ```
> I hope this helps!


> US:
>
> This in not correct, this part [-1] is wrong, the correct way is just:
> 
> knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
> 
> now let's come back to fill the second part of the matrix, you have to fill the other q columns in this way:
> 
> col d+1 -> (x-knot1)^d
> 
> col d+2 -> (x-knot2)^d
> 
> ...
> 
> col d+q -> (x-knotq)^d
> 
> you have to do the power ONLY IF (x-knot) is greater than 0, otherwise you just put 0 in that position


> ChatGPT:
>
> You're right, I apologize for the confusion. Here is the corrected code for creating the knots:
> ```
> knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
> ```
> To fill the second part of the matrix, we need to modify the code inside the nested loop as follows:
> ```
> for (j in 1:d) {
  for (k in 1:q) {
    col_idx <- d + (j-1)*q + k
    basis[, col_idx] <- ifelse(x >= knots[k] & x < knots[k+1],
                               (x-knots[k])^j,
                               0)
  }
}
> ```
> This code computes the index of the current column col_idx and then fills the column with (x-knots[k])^j only when x is within the interval between the k-th and k+1-th knot. If x is outside this interval, the entry is set to 0.
> 
> I hope this resolves the issues and that you are able to get the desired output!


> US:
>
> it is not necessary to do a double loop,  use just one apply to subtract for each new column the correct knot, change the negative value with 0 and then raise to the power of d


> ChatGPT:
>
> You're absolutely right, we can use apply() to simplify the code. Here's an updated version of the code that uses apply() to compute the second part of the matrix:
> ```
> knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
knot_matrix <- matrix(knots, ncol=q, nrow=length(x), byrow=TRUE)
basis <- matrix(1, nrow=length(x), ncol=d+q*d)
# First d columns
basis[, 1:d] <- outer(x, 1:d, FUN = "^")
# Remaining q*d columns
knot_basis <- (x - knot_matrix)
knot_basis[knot_basis < 0] <- 0
basis[, (d+1):(d+q*d)] <- apply(knot_basis, 2, function(col) col^d)
> ```
> Here, we create a knot_matrix that has q columns, each containing the q knots. We then subtract knot_matrix from x to get a knot_basis matrix, and replace negative entries with 0. Finally, we apply the function function(col) col^d to each column of knot_basis to get the desired output for the remaining q*d columns of the basis matrix.
> 
> I hope this updated code works for you!


> US:
>
> the last row 
> 
> basis[, (d+1):(d+q*d)] <- apply(knot_basis, 2, function(col) col^d)
> 
> doesn't work, you have to arrive just to the column d+q

> ChatGPT:
>
> I apologize for the mistake. You are correct, the last row should be modified to arrive just to column d+q.
> 
> Here is the corrected code:
> ```
> trunc_power_basis <- function(x, d, q) {
  knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
  basis <- matrix(1, nrow = length(x), ncol = d + q)
  basis[, 1:d] <- x^(1:d)
  knot_basis <- apply(matrix(x, ncol = q), 2, function(knot) pmax(x - knot, 0)^d)
  basis[, (d+1):(d+q)] <- knot_basis
  return(basis)
}
> ```
> With this implementation, you can now call the function trunc_power_basis with different values of d and q to generate the truncated power basis for various cases.
> 
> For example, to generate the basis for d=1 and q=3, and plot the first five elements of the basis, you can use the following code:
> ```
> basis <- trunc_power_basis(seq(0, 1, length.out = 100), d = 1, q = 3)
plot(seq(0, 1, length.out = 100), basis[,1:5], type = "l", xlab = "x", ylab = "Basis functions")
> ```
> This will produce a plot with the first five basis functions

> US:
>
> This is the function that I have now:
> 
> trunc_power_basis <- function(x, d, q) {
> 
>   knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
> 
>   basis <- matrix(1, nrow = length(x), ncol = d + q)
> 
>   basis[, 1:d] <- outer(x, 1:d, FUN = "^")
> 
>   knot_basis <- (x - knot_matrix)
> 
>   knot_basis[knot_basis < 0] <- 0
> 
>   knot_basis <- apply(matrix(x, ncol = q), 2, function(knot) pmax(x - knot, 0)^d)
> 
>   basis[, (d+1):(d+q)] <- knot_basis
> 
>   return(basis)
> 
> }
> 
> The first part of the function works, you have to change the following code in order to take the d power of each column of knot_basis to fill the remaining q columns of the basis matrix:
> 
> knot_basis <- apply(matrix(x, ncol = q), 2, function(knot) pmax(x - knot, 0)^d)

> ChatGPT:
>
> To take the d power of each column of knot_basis, you can simply add another apply function to the existing code. Here's the updated function:
> ```
> trunc_power_basis <- function(x, d, q) {
  knots <- seq(0, 1, length.out = q+2)[-c(1, q+2)]
  basis <- matrix(1, nrow = length(x), ncol = d + q)
  basis[, 1:d] <- outer(x, 1:d, FUN = "^")
  knot_matrix <- matrix(knots, ncol = q)
  knot_basis <- pmax(outer(x, knots, FUN = "-"), 0)
  knot_basis <- apply(knot_basis, 2, FUN = "^", d)
  basis[, (d+1):(d+q)] <- knot_basis
  return(basis)
}
> ```
> Here, the updated code takes the matrix of differences between x and knots, sets all negative values to 0, and then applies the power d to each column of the resulting matrix using another apply function. The resulting matrix is then added to the basis matrix as the last q columns.

</span>
</details>

### Comments

The total implementation via ChatGPT was "painful", it is useful to use it when you give it detailed explanations to obtain specific commands, when instead the task becomes more complex it fails to provide an accurate code and you have to spend a lot of time (more than we would like to do it yourself) to fix all the errors.

## Kaggle competition {.tabset-fade}

### Hyperparameters selection based on GCV

First of all we define the function for calculating the Generalized Cross Validation (GCV)

```{r message=FALSE, warning=FALSE}

gcv <- function(model, X, true, type, d=NA, q=NA, seed=69, degree=d+q){
  pred <- predict(model, X)
  n    <- length(true)
  if (type == "lm"){
    MSE <- sqrt( (RMSE(pred, true)^2) / (1-(degree+1)/n)^2 )
    return (MSE)
  }
  else if (type == "elnet"){
    df <- model$df
    MSE <- sapply(1:dim(pred)[2], function(x) sqrt( (RMSE(pred[,x], true)^2) / (1-(df[x]+1)/n)^2 ))
    return(c(min(MSE),which.min(MSE)))
  }
  else if (type == "log"){
    pred <- exp(pred)
    MSE <- sqrt( (RMSE(pred, true)^2) / (1-(degree+1)/n)^2 )
    return(MSE)
  }
  else{
    print("Invalid type: please enter 'lm' or 'elnet'.")
    return()
  }
}

```

Looking to the data we can see an increasing spread of the values starting from $x=0.45$ and where $x$ is near to $1$ there are some points that could be consider outliers. We decided to cut them. 

```{r}

train = read.csv("train.csv")
test = read.csv("test.csv")

train$y = scale(train$y, scale=F)
cen = attr(train$y, "scaled:center")

train = train[abs(train$y)<14000, ]
train$y = train$y + cen

```

Now we can start to illustrate the submissions which we account more important.
The first thing that any student might think is the classical linear regression, initializing the grid search of the hyperparameters polynomial degree $d = \{1,2,3\}$ and number of equally spaced knots $q=\{1,\dots,30\}$. To find the best parameters we'll apply the GCV.

```{r message=FALSE, warning=FALSE}

d_vec <- 1:3
q_vec <- 3:30
RMSE_matrix <- matrix(NA, length(d_vec), length(q_vec))

for (d in d_vec){
  for (q in q_vec){
  X <- data.frame(power_basis(train$x,d=d,q_1=q))
  X$y <- train$y
  model <- lm(y ~ ., X)
  RMSE_matrix[d, q-2] <- gcv(model,X,train$y,type="lm",d=d, q=q)
  }
}

best_params <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T), arr.ind = TRUE)

X_test <- data.frame(power_basis(test$x,d=best_params[1], q_1=best_params[2]+2,q_2=0,point=0.45))
X <- data.frame(power_basis(train$x,d=best_params[1], q_1=best_params[2]+2,q_2=0,point=0.45))
X$y <- train$y
model <- lm(y ~ ., X)
target <- predict(model,X_test)

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Vanilla linear regression", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params[1]), size=4) + 
  annotate("text", x=0.067, y=-7500, label=paste("q =", best_params[2]), size=4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

```

We expected it: the prediction is very variable. 

As stated above, the variability of the values starting from $x=0.45$ increase. So between $(0.45,1)$ the likelihood of overfitting increase too: this scenario could be partial dismissed using less knots in this interval. So we redefine the number of knots: $q_1 = \{1,\dots,20\}$ in the first part and $q_2 = \{1,\dots,5\}$ in the second. 

```{r message=FALSE, warning=FALSE}

d_vec <- 1:3
q_1_vec <- 1:20
q_2_vec <- 1:5
RMSE_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec)))

for (d in d_vec){
  for (q_1 in q_1_vec){
    for (q_2 in q_2_vec[q_2_vec<=q_1]){
      X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
      X$y <- train$y
      model <- lm(y ~ ., X)
      RMSE_matrix[d, q_1, q_2] <- gcv(model, X, type = "lm", train$y, d=d, q=q_1+q_2)
    }
  }
}

best_params <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T),arr.ind = TRUE)

X_test <- data.frame(power_basis(test$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
X <- data.frame(power_basis(train$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
X$y <- train$y
model <- lm(y ~ ., X)
target <- predict(model,X_test)

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Linear regression (not equi-spaced)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

```

An other way to handle the increase of spread is to weight less the points which lie in the part more variable. So we had estimate the "moving" standard deviation with window size 20 points (we had choose an unique value because the distribution of $x$ seems uniform) and filling the initial and final empty values with the first and last.

```{r message=FALSE, warning=FALSE}

train = train[order(train$x),]
sd_est <- sapply(11:661, function(x) sd(train$y[x-10:x+10]))
sd_est <- c(rep(sd_est[1],10),sd_est,rep(sd_est[660], 10))
sd_est <- ((1/sd_est)^3)

d_vec <- 1:3
q_1_vec <- 1:20
q_2_vec <- 1:5
RMSE_matrix <- array(NA, dim = c(length(d_vec), length(q_1_vec), length(q_2_vec)))

for (d in d_vec){
  for (q_1 in q_1_vec){
    for (q_2 in q_2_vec[q_2_vec<=q_1]){
      X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
      X$y <- train$y
      model <- lm(y ~ ., X, weights = sd_est)
      RMSE_matrix[d, q_1, q_2] <- gcv(model, X, type = "lm", train$y, d=d, q=q_1+q_2)
    }
  }
}

best_params <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T),arr.ind = TRUE)

X_test <- data.frame(power_basis(test$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
X <- data.frame(power_basis(train$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
X$y <- train$y
model <- lm(y ~ ., X, weights = sd_est)
target <- predict(model,X_test)

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Linear regression (weigthed)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

```

We could try to fit some penalized regression: in this sub we'll train a elastic net model. 

```{r message=FALSE, warning=FALSE}

d_vec <- 1:3
q_1_vec <- 1:20
q_2_vec <- 1:5
alpha_vec <- seq(0,1,0.1)
lambda_vec <- seq(0,0.0005,0.00003)
RMSE_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec),length(alpha_vec)))
lam_res <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec),length(alpha_vec)))

for (d in d_vec){
  for (q_1 in q_1_vec){
    for (q_2 in q_2_vec[q_2_vec<=q_1]){
      for (a in alpha_vec){
          X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
          model <- glmnet(X, train$y, alpha=a, lambda=lambda_vec)
          GCV <- gcv(model, as.matrix(X), type="elnet", train$y, degree=model$df)
          RMSE_matrix[d, q_1, q_2, a*10+1] <- GCV[1]
          lam_res[d, q_1, q_2, a*10+1] <- GCV[2] 
      }
    }
  }
}

best_params <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T),arr.ind = TRUE)

X_test <- data.frame(power_basis(test$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
X <- data.frame(power_basis(train$x,d=best_params[1],q_1=best_params[2],q_2=best_params[3],point=0.45))
model <- glmnet(X,train$y,alpha=a,lambda=lam_res[best_params[1], best_params[2], best_params[3], best_params[3]])
target <- as.numeric(predict(model,as.matrix(X_test)))

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Elastic net", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face="bold"),
        legend.text = element_text(size = 9))

```

It doesn't seem to look good... we will throw it away.

An important model feature in this kind of problem is the robustness. The following two models are been implemented to improve this condition: the first leverages on a non-linear data transformation, the logarithm, and the second is a linear regression but fitted using MAE.

```{r message=FALSE, warning=FALSE}

d_vec <- 1:3
q_1_vec <- 1:20
q_2_vec <- 1:5
RMSE_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec)))

m <- -min(train$y)*2
train_log <- train$y + m

for (d in d_vec){
  for (q_1 in q_1_vec){
    for (q_2 in q_2_vec[q_2_vec<=q_1]){
          X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
          model <- lm(log(train_log)~., X)
          GCV <- gcv(model, X, type = "log", train_log, d=d, q=q_1+q_2)
          RMSE_matrix[d, q_1, q_2] <- GCV
    }
  }
}


best_params_log <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T),arr.ind = TRUE)

X <- data.frame(power_basis(train$x,d=best_params_log[1],q_1=best_params_log[2],q_2=best_params_log[3],point=0.45))
X_test <- data.frame(power_basis(test$x,d=best_params_log[1],q_1=best_params_log[2],q_2=best_params_log[3],point=0.45))
model <- lm(log(train_log) ~ ., X)

target <- exp(predict(model,X_test)) - m

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Log linear regression", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params_log[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params_log[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params_log[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

```

```{r message=FALSE, warning=FALSE}

MAESummary <- function (data, lev = NULL, model = NULL) 
{
  out <- MAE(data$obs,data$pred, 
             na.rm = TRUE)  
  names(out) <- "MAE"
  out
}

d_vec <- 1:3
q_1_vec <- 1:20
q_2_vec <- 1:5
RMSE_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec)))


for (d in d_vec){
  for (q_1 in q_1_vec){
    for (q_2 in q_2_vec[q_2_vec<=q_1]){
          X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
          X$y <- train$y
          model <- train(y ~ ., 
                         data = X, 
                         method = "lm",
                         metric = "MAE",
                         trControl = trainControl(summaryFunction = MAESummary, method="none"))        
          GCV <- gcv(model, X, type = "lm", train$y, d=d, q=q_1+q_2)
          RMSE_matrix[d, q_1, q_2] <- GCV
    }
  }
}


best_params_MAE <- which(RMSE_matrix==min(RMSE_matrix,na.rm=T), arr.ind = TRUE)

X <- data.frame(power_basis(train$x,d=best_params_MAE[1],q_1=best_params_MAE[2],q_2=best_params_MAE[3],point=0.45))
X_test <- data.frame(power_basis(test$x,d=best_params_MAE[1],q_1=best_params_MAE[2],q_2=best_params_MAE[3],point=0.45))
X$y <- train$y
model <- train(y ~ .,
               data = X, 
               method = "lm",
               metric = "MAE",
               trControl = trainControl(summaryFunction = MAESummary, method="none"))

target <- predict(model,X_test)

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="b")
df <- rbind(df_train, df_test)

ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="b"), alpha=0.8, size=1.5, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2"), labels=c("Dataset", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Linear regression (MAE)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.059, y=-6500, label=paste("d =", best_params_MAE[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params_MAE[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params_MAE[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

```


### Nested Cross-Validation implementation

In order to implement the Nested Cross-Validation we based our function on the pseudo-code described by [Bates et al. (2022)](https://arxiv.org/pdf/2104.00673.pdf).

This is the code that we wrote:

```{r message=FALSE, warning=FALSE}

ncv <- function(df, R, K, type){

  library(caret)
  
  # initialize progress bar
  pb <- txtProgressBar(min=0, max=R*K, style=3)
  
  # set.seed(seed)
  
  # df <-- dataset composed of the design matrix 'X' and the target 'y' as a last column
  # R  <-- number of repetitions
  # K  <-- number of folds
  
  # initialize the variables
  es     <- rep(NA, R*K)
  a_list <- rep(NA, R*K)
  b_list <- rep(NA, R*K)
  
  # loop trough the simulation size
  for (r in 1:R){
  
    # randomly assign points to folds I1,...,IK
    folds_id <- createFolds(1:nrow(df), k=K)
    folds_df <- lapply(folds_id, function (f) df[f, ])

    # loop trough the number of folds
    for (k in 1:K){
      
      # increment progress bar
      pos <- k + K*(r-1)
      setTxtProgressBar(pb, pos)
      
      # calculate inner cross-validation
      e_in <- icv(df=folds_df, k=k, type=type)
      
      # dataframe without the k-th fold
      df_sub <- data.frame(do.call(rbind, lapply(seq_along(folds_df)[-k], function(i) folds_df[[i]])))
      
      # fit the algorithm
      if (type=="MAE"){
        fit <- train(y ~ ., 
                     data = df_sub, 
                     method = "lm",
                     metric = "MAE",
                     trControl = trainControl(summaryFunction = MAESummary, method="none")) 
      }
      else if(type=="log"){
        fit <- lm(df_sub[,ncol(df_sub)] ~ ., df_sub[,-ncol(df_sub)])
      }
      
      # dataframe with only the k-th fold
      df_k <- data.frame(do.call(rbind, lapply(seq_along(folds_df)[k], function(i) folds_df[[i]])))
      
      # predict the labels for the k-th fold
      if (type=="MAE"){
        pred <- as.vector(predict(fit, newdata=df_k))
        y_true <- as.vector(df_k[,ncol(df_k)])
        e_out <- (y_true-pred)^2
      }
      else if(type=="log"){
        pred <- exp(as.vector(predict(fit, newdata=df_k)))
        y_true <- exp(as.vector(df_k[,ncol(df_k)]))
        e_out <- (y_true-pred)^2
      }
      
      # compute the square loss for each point
      a_list[pos] <- (mean(e_in) - mean(e_out))^2
      b_list[pos] <- var(e_out) / nrow(df_k)
      es[pos] <- mean(e_in)
      
    }
  }
  close(pb)
  
  return (cbind(es,a_list,b_list))
  
}


# Inner cross-validation subroutine
icv <- function(df, k_in, type){
  
  # df <-- dataset composed of the design matrix 'X' and the target 'y' as a last column without the k-th fold
  # K  <-- is already K-1
  
  # initialize the variable
  e_in <- 0
  
  # loop trough the number of folds
  for (k in 1:K){
    
    if (k != k_in){
      # dataframe without the k-th fold
      df_sub <- data.frame(do.call(rbind, lapply(seq_along(df)[-c(k,k_in)], function(i) df[[i]])))
      # fit the algorithm
        if (type=="MAE"){
          fit <- train(y ~ ., 
                       data = df_sub, 
                       method = "lm",
                       metric = "MAE",
                       trControl = trainControl(summaryFunction = MAESummary, method="none")) 
        }
        else if(type=="log"){
          fit <- lm(df_sub[,ncol(df_sub)] ~ ., df_sub[,-ncol(df_sub)])
        }
      
      # dataframe with only the k-th fold
      df_k <- data.frame(do.call(rbind, lapply(seq_along(df)[k], function(i) df[[i]])))
      
      # predict the labels for the k-th fold
      if (type=="MAE"){
        pred <- as.vector(predict(fit, newdata=df_k[,-ncol(df_k)]))
        y_true <- as.vector(df_k[, ncol(df_k)])
        e_in <- c(e_in, (y_true-pred)^2)
      }
      else if(type=="log"){
        pred <- exp(as.vector(predict(fit, newdata=df_k[,-ncol(df_k)])))
        y_true <- exp(as.vector(df_k[, ncol(df_k)]))
        e_in <- c(e_in, (y_true-pred)^2)
      }
    }
  }
  return (e_in[-1])
}
```

### Hyperparameters selection based on NCV

Given that the best results are likely assured by the more robust models and that the NCV is very computationally expensive, we'd decided to run the NCV only on the last two models, which we'll be our final choices for the competition.

```{r eval=F, echo=T}

set.seed(69)

numWorkers <- detectCores()  # Use all available CPUs
cl <- makeCluster(numWorkers)  # Create a cluster with the specified number of workers
registerDoParallel(cl)  # Register the parallel backend

R <- 200
K <- 5
d_vec <- 1:3
q_1_vec <- 2:12
q_2_vec <- 1:5
RMSE_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec)))
ERR_matrix <- array(NA, dim = c(length(d_vec),length(q_1_vec),length(q_2_vec)))

m <- - min(train$y) + 1
train_log <- train$y + m


for (d in d_vec){
  for (q_1 in q_1_vec){
    print(q_1)
    for (q_2 in q_2_vec){
    X <- data.frame(power_basis(train$x,d=d,q_1=q_1,q_2=q_2,point=0.45))
    X$y <- log(train_log)
    for_value <- foreach(i=1:numWorkers, .combine=rbind) %dopar% ncv(df=X, R=as.integer(R/numWorkers), K=K, type="log")
    RMSE_matrix[d,q_1-1, q_2] <-  sqrt(mean(for_value[,2])-mean(for_value[,3]))
    ERR_matrix[d,q_1-1, q_2] <- sqrt(mean(for_value[,1]))
    }
  }
}

#save(RMSE_matrix,ERR_matrix,file="log_ncv.Rdata")

stopCluster(cl)

```


```{r fig.height=15, fig.width=12, message=FALSE, warning=FALSE}

X <- data.frame(power_basis(train$x,d=best_params_log[1],q_1=best_params_log[2],q_2=best_params_log[3],point=0.45))
X_test <- data.frame(power_basis(test$x,d=best_params_log[1],q_1=best_params_log[2],q_2=best_params_log[3],point=0.45))
model <- lm(log(train_log)~., X)

target <- exp(predict(model,X_test)) - m
pred_train <- exp(predict(model,X)) - m
df_train_RMSE <- data.frame(x=train$x, y=pred_train, group="b")
df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="c")
df <- rbind(df_train, df_train_RMSE, df_test)

plot5 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_train_RMSE, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Logarithm (GCV)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_params_log[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params_log[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params_log[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))



X <- data.frame(power_basis(train$x,d=best_params_MAE[1],q_1=best_params_MAE[2],q_2=best_params_MAE[3],point=0.45))
X_test <- data.frame(power_basis(test$x,d=best_params_MAE[1],q_1=best_params_MAE[2],q_2=best_params_MAE[3],point=0.45))
X$y <- train$y
model <- train(y ~ .,
               data = X, 
               method = "lm",
               metric = "MAE",
               trControl = trainControl(summaryFunction = MAESummary, method="none"))

target <- predict(model,X_test)
pred_train <- predict(model,X)
df_train_RMSE <- data.frame(x=train$x, y=pred_train, group="b")
df_train <- data.frame(x=train$x, y=train$y, group="a")
df_test <- data.frame(x=test$x, y=target, group="c")
df <- rbind(df_train, df_train_RMSE, df_test)

plot6 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_train_RMSE, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="MAE (GCV)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_params_MAE[1]), size=4) + 
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_params_MAE[2]), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_params_MAE[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face="bold"),
        legend.text = element_text(size = 9))


load("log_ncv.Rdata")

best_RMSE <- which(RMSE_matrix==min(RMSE_matrix, na.rm = T), arr.ind=TRUE)
best_ERR <- which(ERR_matrix==min(ERR_matrix), arr.ind=TRUE)

X_RMSE <- data.frame(power_basis(train$x,d=best_RMSE[1],q_1=best_RMSE[2]+1,q_2=best_RMSE[3],point=0.45))
X_ERR <- data.frame(power_basis(train$x,d=best_ERR[1],q_1=best_ERR[2]+1,q_2=best_ERR[3],point=0.45))
X_test_RMSE <- data.frame(power_basis(test$x,d=best_RMSE[1],q_1=best_RMSE[2]+1,q_2=best_RMSE[3],point=0.45))
X_test_ERR <- data.frame(power_basis(test$x,d=best_ERR[1],q_1=best_ERR[2]+1,q_2=best_ERR[3],point=0.45))
model_RMSE <- lm(log(train_log)~., X_RMSE)
model_ERR <- lm(log(train_log)~., X_ERR)
pred_train_RMSE <- exp(predict(model_RMSE,X_RMSE)) - m
pred_train_ERR <- exp(predict(model_ERR,X_ERR)) - m
target_RMSE <- exp(predict(model_RMSE,X_test_RMSE)) - m
target_ERR <- exp(predict(model_ERR,X_test_ERR)) - m

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_train_RMSE <- data.frame(x=train$x, y=pred_train_RMSE, group="b")
df_test_RMSE <- data.frame(x=test$x, y=target_RMSE, group="c")
df <- rbind(df_train, df_train_RMSE, df_test_RMSE)

plot1 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_test_RMSE, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test_RMSE, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Logarithm (NCV-RMSE)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_RMSE[1]), size=4) +
  annotate("text", x=0.082, y=-7500, label=paste("q_1 =", best_RMSE[2]+1), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_RMSE[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

df_train_ERR <- data.frame(x=train$x, y=pred_train_ERR, group="b")
df_test_ERR <- data.frame(x=test$x, y=target_ERR, group="c")
df <- rbind(df_train, df_train_ERR, df_test_ERR)

plot2 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_train_ERR, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test_ERR, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="Logarithm (NCV-ERR)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_ERR[1]), size=4) +
  annotate("text", x=0.082, y=-7500, label=paste("q_1 =", best_ERR[2]+1), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_ERR[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

load("MAE_ncv.Rdata")

best_RMSE <- which(RMSE_matrix==min(RMSE_matrix), arr.ind=TRUE)
best_ERR <- which(ERR_matrix==min(ERR_matrix), arr.ind=TRUE)

X_RMSE <- data.frame(power_basis(train$x,d=best_RMSE[1],q_1=best_RMSE[2]+1,q_2=best_RMSE[3],point=0.45))
X_ERR <- data.frame(power_basis(train$x,d=best_ERR[1],q_1=best_ERR[2]+1,q_2=best_ERR[3],point=0.45))
X_test_RMSE <- data.frame(power_basis(test$x,d=best_RMSE[1],q_1=best_RMSE[2]+1,q_2=best_RMSE[3],point=0.45))
X_test_ERR <- data.frame(power_basis(test$x,d=best_ERR[1],q_1=best_ERR[2]+1,q_2=best_ERR[3],point=0.45))

X_RMSE$y <- train$y
X_ERR$y <- train$y
model_RMSE <- train(y ~ ., 
                    data = X_RMSE, 
                    method = "lm",
                    metric = "MAE",
                    trControl = trainControl(summaryFunction = MAESummary, method="none"))
model_ERR <- train(y ~ ., 
                   data = X_ERR, 
                   method = "lm",
                   metric = "MAE",
                   trControl = trainControl(summaryFunction = MAESummary, method="none"))
pred_train_RMSE <- predict(model_RMSE,X_RMSE)
pred_train_ERR <- predict(model_ERR,X_ERR)
target_RMSE <- predict(model_RMSE,X_test_RMSE)
target_ERR <- predict(model_ERR,X_test_ERR)

df_train <- data.frame(x=train$x, y=train$y, group="a")
df_train_RMSE <- data.frame(x=train$x, y=pred_train_RMSE, group="b")
df_test_RMSE <- data.frame(x=test$x, y=target_RMSE, group="c")
df <- rbind(df_train, df_train_RMSE, df_test_RMSE)

plot3 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_train_RMSE, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test_RMSE, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="MAE (NCV-RMSE)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_RMSE[1]), size=4) +
  annotate("text", x=0.075, y=-7500, label=paste("q_1 =", best_RMSE[2]+1), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_RMSE[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

df_train_ERR <- data.frame(x=train$x, y=pred_train_ERR, group="b")
df_test_ERR <- data.frame(x=test$x, y=target_ERR, group="c")
df <- rbind(df_train, df_train_ERR, df_test_ERR)

plot4 <- ggplot(df_train, aes(x=x, y=y)) +
  geom_point(aes(color="a"), alpha=0.6, size=2.5, shape=19) +
  geom_point(data=df_train_ERR, aes(x=x, y=y, color="b"), alpha=0.6, size=2, shape=19) +
  geom_point(data=df_test_ERR, aes(x=x, y=y, color="c"), alpha=0.8, size=1, shape=19) +
  scale_color_manual(values=c("#F0863A", "#0072B2", "#FF0000"), labels=c("Dataset", "Prediction on Training Set", "Prediction on Test Set")) +
  theme_bw() +
  labs(title="MAE (NCV-ERR)", x="x", y="y", color="Legend") +
  annotate("text", x=0.102, y=-5000, label="Parameters:", size=4, fontface="bold") + 
  annotate("text", x=0.055, y=-6500, label=paste("d =", best_ERR[1]), size=4) +
  annotate("text", x=0.082, y=-7500, label=paste("q_1 =", best_ERR[2]+1), size=4) +
  annotate("text", x=0.075, y=-8500, label=paste("q_2 =", best_ERR[3]), size = 4) +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(color = "black", fill = "white"),
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9))

grid.arrange(plot1, plot3, plot2, plot4, plot5, plot6, ncol=2)

```

### Why do we need Nested Cross Validation? 
When we adopt the classical cross validation we're assuming that the errors which come up by the split-train cycle are independent. This assumption makes the confidence intervals narrower then what should be. The nested CV tries to estimate better the variability of the error.

### Models comparison

Visually the shape of the prediction are quite similar. The choice of not equi-spaced knots is right: indeed in all the cases the best number of knots in the interval $(0.45,1)$ is minor then $2$.
Starting from the GCV, the best parameters for both the models are the same and they are somewhat low, indeed with this type of cross-validation increasing degrees of freedom, so $d$, $q_1$ and $q_2$, affect a lot the final score. Probably GCV prefers models too much simple, at least in our case.
Looking to the other results we find higher parameters, particularly choosing the model with the "1-SE" method the polynomial degree is $3$ in both the case: the smoother is the shape the less variable is the error. Not to be surprised about the polynomial degree using MAE measure: in fact the minimum error is found with $d=1$, obviously in this case the better choice is the "1-SE" method. For what about the logarithm submission we'll try the "stranger result", this because we want to submit a model that in the chunk near $1$ goes down to differentiate it from the one based on MAE and since from the peeks we were allowed on the private ranking we understood that it could be a good idea.
