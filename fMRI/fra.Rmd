---
title: "fra"
author: "Io"
date: "2023-05-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carico i dati

```{r }

train <- read.csv("Data/train_hw03.csv")
test  <- read.csv("Data/test_hw03.csv")

train$y = ifelse(train$y=="autism", 1, 0)
train$sex = ifelse(train$sex=="male", 1, 0)

test$sex = ifelse(test$sex=="male", 1, 0)

```

```{r }
N_ROIs <- 116
ROI_length <- 115

train_partial <- data.frame(matrix(ncol=N_ROIs*2, nrow=nrow(train)))
col_names <- c(paste0("mean_ROI_", 1:N_ROIs), paste0("median_ROI_", 1:N_ROIs))
colnames(train_partial) <- col_names

start <- 5

for (i in 1:(N_ROIs)){
  
  sub_set <- train[, start:(start+ROI_length-1)]
  ROIs_mean <- apply(sub_set, MARGIN=1, FUN=mean)
  # ROIs_sd   <- apply(sub_set, MARGIN=1, FUN=sd)
  ROIs_median <- apply(sub_set, MARGIN=1, FUN=median)
  train_partial[, i] <- ROIs_mean
  train_partial[, i+N_ROIs] <- ROIs_median
  start <- start + ROI_length
  
}

train_new <- cbind(train[, 1:4], train_partial)
```


```{r }
N_ROIs <- 116
ROI_length <- 115

test_partial <- data.frame(matrix(ncol=N_ROIs*2, nrow=nrow(test)))
col_names <- c(paste0("mean_ROI_", 1:N_ROIs), paste0("sd_ROI_", 1:N_ROIs))
colnames(test_partial) <- col_names

start <- 4

for (i in 1:(N_ROIs)){
  
  sub_set <- test[, start:(start+ROI_length-1)]
  ROIs_mean <- apply(sub_set, MARGIN=1, FUN=mean)
  ROIs_sd   <- apply(sub_set, MARGIN=1, FUN=sd)
  test_partial[, i] <- ROIs_mean
  test_partial[, i+N_ROIs] <- ROIs_sd
  start <- start + ROI_length
  
}

test_new <- cbind(test[, 1:3], test_partial)
```


```{r }
set.seed(123)

N <- nrow(train)
n1 <- 401
idx <- sample(1:N, n1, replace=F)
```


```{r }
Dn_1 <- train_new[idx, ]
Dn_2 <- train_new[-idx, ]
```

```{r}
model = glm(factor(y) ~ . -id, family = binomial(link = "logit"), data = Dn_1)
summary(model)
predicts <- predict(model, newdata = Dn_2, type = "response")
binary_predictions <- ifelse(predicts >= 0.5, 1, 0)
cm = table(binary_predictions,Dn_2$y)
classification_err <- 1 - sum(diag(cm)) / sum(cm)
classification_err
```

```{r warning=FALSE}
library(randomForest)

modello_forest <- randomForest(factor(y) ~ . -id, data = Dn_1, importance = TRUE)
summary(modello_forest)

predictions <- predict(modello_forest, newdata = Dn_2)
table(predictions)
table(Dn_2$y)

cm = table(predictions,Dn_2$y)
classification_err <- 1 - sum(diag(cm)) / sum(cm)
classification_err
```


```{r}
importance_variables <- importance(modello_forest)
importance_variables = importance_variables[order(importance_variables, decreasing = TRUE),]
head(importance_variables,10)

Dn_1_most_importance <- data.frame(y = Dn_1$y,
                              median_ROI_46 = Dn_1$median_ROI_46,
                              mean_ROI_44 = Dn_1$mean_ROI_44,
                              median_ROI_89 = Dn_1$median_ROI_89,
                              median_ROI_5 = Dn_1$median_ROI_5,
                              median_ROI_34 = Dn_1$median_ROI_34,
                              mean_ROI_36 = Dn_1$mean_ROI_36,
                              mean_ROI_20 = Dn_1$mean_ROI_20,
                              mean_ROI_94 = Dn_1$mean_ROI_94,
                              mean_ROI_2 = Dn_1$mean_ROI_2,
                              median_ROI_104 = Dn_1$median_ROI_104)

Dn_2_most_importance <- data.frame(y = Dn_2$y,
                              median_ROI_46 = Dn_2$median_ROI_46,
                              mean_ROI_44 = Dn_2$mean_ROI_44,
                              median_ROI_89 = Dn_2$median_ROI_89,
                              median_ROI_5 = Dn_2$median_ROI_5,
                              median_ROI_34 = Dn_2$median_ROI_34,
                              mean_ROI_36 = Dn_2$mean_ROI_36,
                              mean_ROI_20 = Dn_2$mean_ROI_20,
                              mean_ROI_94 = Dn_2$mean_ROI_94,
                              mean_ROI_2 = Dn_2$mean_ROI_2,
                              median_ROI_104 = Dn_2$median_ROI_104)
```


```{r}
forest_top10 <- randomForest(factor(y) ~ . , data = Dn_1_most_importance)

predictions <- predict(forest_top10, newdata = Dn_2_most_importance)
table(predictions)
```
```{r}
cm = table(predictions,Dn_2$y)
cm
classification_err <- 1 - sum(diag(cm)) / sum(cm)
classification_err
```
```{r}
library(caret)
Dn_1$y <- factor(Dn_1$y)

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 2, search = "random")

# Definisci i valori dei parametri desiderati
parametri <- data.frame(
  nrounds = c(100,150,175,200,225,250,300),  # Numero di iterazioni
  max_depth = c(4,5,6,7,8,9,10),      # Profondità massima dell'albero
  eta = c(0.01, 0.05, 0.1,0.15, 0.2, 0.25, 0.3),      # Learning rate
  gamma = c(0, 0.05, 0.1,0.15, 0.2, 0.25, 0.3),
  subsample = c(0.4, 0.5,0.55, 0.6, 0.65, 0.7, 0.8),
  colsample_bytree = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
  min_child_weight = c(1, 3, 5, 7, 8, 9, 10)
)

# Crea il controllo per la grid search
ctrl <- trainControl(
  method = "cv",              # Utilizza la cross-validation
  number = 5,                 # Numero di folds per la cross-validation
  verboseIter = TRUE          # Stampa informazioni durante la ricerca
)


# Esegui la grid search
model_grid <- train(
  y ~ . -id,
  data = Dn_1,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = parametri
)

combinazioni_parametri <- expand.grid(parametri)

miglior_modello <- NULL
accuratezza_massima <- 0

# Itera su ciascuna combinazione di parametri
for (i in 1:nrow(combinazioni_parametri)) {
  # Seleziona la combinazione corrente di parametri
  parametri_correnti <- combinazioni_parametri[i, ]
  
  # Addestra il modello con la combinazione corrente di parametri
  model_tune_xgb <- train(
    y ~ . -id,
    data = Dn_1,
    method = "xgbTree",
    trControl = ctrl,
    tuneGrid = parametri_correnti
  )
  
  # Esegui le previsioni sul dataset Dn_2
  preds <- predict(model_tune_xgb, newdata = Dn_2)
  
  # Calcola l'accuratezza
  cm <- confusionMatrix(data = preds, factor(Dn_2$y))
  accuracy <- as.numeric(cm$overall[1])
  
  # Verifica se l'accuratezza corrente è superiore all'accuratezza massima
  if (accuracy > accuratezza_massima) {
    accuratezza_massima <- accuracy
    miglior_modello <- model_tune_xgb
  }
}

# Visualizza il miglior modello e l'accuratezza massima
print(miglior_modello)
print(accuratezza_massima)

```


```{r}
preds = ifelse(preds == "0", "control", "autism")
```

```{r}
library(xgboost)
X_train <- as.matrix(Dn_1[,-4])
y_train <- Dn_1$y
X_test <- as.matrix(Dn_2[,-4])
X_valid <- as.matrix(test_new)

modello_xgboost <- xgboost(data = X_train, 
                           label = y_train, 
                           nrounds = 100)

preds <- predict(modello_xgboost, X_test)
preds = ifelse(preds >=1.5,1,0)
table(preds)
confusionMatrix(factor(preds), factor(Dn_2$y))

prev = predict(modello_xgboost, X_valid)
prev = ifelse(prev >=1.28,1,0)
table(prev)
```
```{r}

y_test = data.frame(id = test_new$id,target = preds)
write.csv(y_test, file = "prova1.csv", row.names = F)
```

