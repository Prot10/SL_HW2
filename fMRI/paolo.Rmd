---
title: "Homework 2 Stat-Learn"
author: "Leoni, Morosini, Protani, Sottile, Tarantino"
date: "06/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(doParallel)
library(boot)
library(randomForest)
library(glmnet)
library(ppcor)
library(ggplot2)
library(gt)
library(gganimate)
library(grid)
library(jpeg)
```

# Introduction
In this homework we'll deal with fRMI, in particular with the Autism Brain Image Data Exchange Project. Our train data are time series of activity associated to 116 distinct ROIs (regions of interest) observed at 115 time instants. We have to keep up some information by this data to classify autism and "normal" brain.

```{r }

load("coordinates.RData")
train <- read.csv("train_hw03.csv")
train$y <- ifelse(train$y=="autism",1,0)

coordinates <- aal116coordinates[, 2:4]

N_ROIs <- 116
ROI_length <- 115

```

#### Simulazione grafica di una serie temporale

```{r}
subset_td  <- train[train$y == 0, 5:ncol(train)]
subset_asd <- train[train$y == 1, 5:ncol(train)]

# Calculate the mean over the rows of each column (except class column)
mean_td_subject  <- data.frame(colMeans(subset_td))
mean_asd_subject <- data.frame(colMeans(subset_asd))

df_td  <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))
df_asd <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

```

Grafico td:

<div style="text-align: center;">

```{r, echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold'}

### axial coordinates parameters

dec_factor <- 1.65
max_x <- max(coordinates$x) * dec_factor
min_x <- min(coordinates$x) * dec_factor
max_y <- max(coordinates$y) * dec_factor
min_y <- min(coordinates$y) * (dec_factor-0.35)

################
### Axial TD ###
################

axial_td  <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "axialTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(axial_td, aes(x=x, y=y-10)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)), color="chartreuse", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)


#################
### Axial ASD ###
#################

axial_asd <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "axialASD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(axial_asd, aes(x=x, y=y-10)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)), color="chartreuse", shape = 15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)


### coronal coordinates parameters

dec_factor <- 1.65
max_x <- max(coordinates$x) * (dec_factor-0.4)
min_x <- min(coordinates$x) * (dec_factor-0.4)
max_y <- max(coordinates$z) * (dec_factor)
min_y <- min(coordinates$z) * (dec_factor)

##################
### Coronal TD ###
##################

coronal_td  <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "coronalTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(coronal_td, aes(x=x, y=y)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)


###################
### Coronal ASD ###
###################

coronal_asd <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "coronalASD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(coronal_asd, aes(x=x, y=y)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)


### sagittal coordinates parameters

dec_factor <- 1.65
max_x <- max(coordinates$y) * (dec_factor-0.5)
min_x <- min(coordinates$y) * (dec_factor-0.5)
max_y <- max(coordinates$z) * (dec_factor)
min_y <- min(coordinates$z) * (dec_factor)

###################
### Sagittal TD ###
###################

sagittal_td  <- data.frame(cbind(x=rep(-coordinates$y, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "sagittalTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(sagittal_td, aes(x=x-35, y=y)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)


####################
### Sagittal ASD ###
####################

sagittal_asd <- data.frame(cbind(x=rep(-coordinates$y, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

# Path to the image file
image_path <- "sagittalASD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(sagittal_asd, aes(x=x-35, y=y)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot, width=350, height=350)

```

</div>

#### Axial view

```{r , eval=FALSE}

axial_td  <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))
axial_asd <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$y, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

dec_factor <- 1.65
max_x <- max(coordinates$x) * (dec_factor+0.15)
min_x <- min(coordinates$x) * (dec_factor+0.15)
max_y <- max(coordinates$y) * (dec_factor)
min_y <- min(coordinates$y) * (dec_factor-0.35)

# Path to the image file
image_path <- "axialTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(axial_td, aes(x=x, y=y-10)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot)

```

#### Coronal view

```{r , eval=FALSE}

coronal_td  <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))
coronal_asd <- data.frame(cbind(x=rep(coordinates$x, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

dec_factor <- 1.65
max_x <- max(coordinates$x) * (dec_factor-0.4)
min_x <- min(coordinates$x) * (dec_factor-0.4)
max_y <- max(coordinates$z) * (dec_factor)
min_y <- min(coordinates$z) * (dec_factor)

# Path to the image file
image_path <- "coronalTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(coronal_td, aes(x=x, y=y+15)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot)

```

#### Sagittal view

```{r , eval=FALSE}

sagittal_td  <- data.frame(cbind(x=rep(-coordinates$y, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_td_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))
sagittal_asd <- data.frame(cbind(x=rep(-coordinates$y, each=ROI_length), y=rep(coordinates$z, each=ROI_length), 
                    values=mean_asd_subject[, 1], time_step=rep(1:ROI_length, N_ROIs)))

dec_factor <- 1.65
max_x <- max(coordinates$y) * (dec_factor-0.5)
min_x <- min(coordinates$y) * (dec_factor-0.5)
max_y <- max(coordinates$z) * (dec_factor)
min_y <- min(coordinates$z) * (dec_factor)

# Path to the image file
image_path <- "sagittalTD.jpg"

# Read the JPEG image
image_data <- readJPEG(image_path)

# Plot the animated points with the image as background
p <- ggplot(sagittal_td, aes(x=x-35, y=y)) +
  annotation_custom(
    rasterGrob(image_data, width=unit(1, "npc"), height=unit(1, "npc")),
    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf
  ) +
  geom_point(aes(size=abs(values), alpha=abs(values)+0.5), color="yellow", shape=15) +
  labs(x=NULL, y=NULL) +
  theme_void() +
  xlim(min_x, max_x) +
  ylim(min_y, max_y) +
  theme(legend.position="none")

# Create the animated plot
animated_plot <- p +
  transition_time(time_step) +
  ease_aes("linear")

# Render and play the animation
animate(animated_plot)
```

# Feature extraction

The biggest problem we encountered is the data preprocess and feature extraction. By a quik look to the data we find no NA, but the data are very different in scales: there are some patients whose data of the order of unity and other are represented by data of the order of miles.
Initially we would have liked to rescale or standardize to solve this problem. An other approach, we'd used it, is to go through correlations: in this way, we avoid the problem.

So, let's go more in depth in our feature extraction: 
1. First of all we have created a matrix for each patient which contains all the data values (row = times, col = ROIs).
2. We're thinking that 115 realizations are to many to find a good value for the correlation. Trying to solve this problem we want to approximate the correlation coefficients calculating them for the firsts 25 events and then scaling by 10 ones, until the end of the data. So take the element-wise median of these matrices.
3. Finally, take the lower triangle values: we'll use these in our analysis.

WHY? Nobody knows.

```{r}
# Number of ROIs
N_ROIs <- 116
# Number of realizations
ROI_length <- 115

# Build ROIs matrices 
mod <- list()
for (i in 1:600){
  mod[[i]] <- matrix(nrow = ROI_length, ncol = N_ROIs)
}

# Fill ROIs matrices
start <- 5
for (i in 1:(N_ROIs)){
  sub_set <- train[, start:(start+ROI_length-1)]
  for (j in 1:600){ 
    mod[[j]][,i] <- as.numeric(sub_set[j,])
  }
  start <- start + ROI_length
}

# Calculate correlation coefficients
mat_corr <- matrix(nrow = 600, ncol = 116*115/2+1)
mat_c <- matrix(0,116,116)
for (j in 1:600){
  # 10 correlation matrices, scaling by 10 
  l <- lapply(1:10, function(x) suppressWarnings(cor(mod[[j]][(10*(x-1)+1):(25+10*(x-1)),])))
  mat_c <- apply(simplify2array(l),MARGIN=1:2,median)
  mat_corr[j,1:6670] <- mat_c[lower.tri(mat_c)]
}

# Fill with 0 NA values
mat_corr[,1:6670] <- ifelse(is.na(mat_corr[,1:6670]),0,mat_corr[,1:6670])
# Add age
mat_corr[,6671] <- train$age

# Split in two part: they'll be used soon
D_1 <- mat_corr[1:400,]
D_2 <- mat_corr[401:600,]
lab_D_1 <- train$y[1:400]
lab_D_2 <- train$y[401:600]
```

We have a lot of features... Let's cut some useless deadwoods.

Elastic-net technique is used also in classification. In this case it'll be exploited to find the most relevant features. More in detail, a Elastic-net model's going to be fitted and in the next analyses only the features take in consideration by this model will be used. We don't know if it makes sense, so we're trying.

Observation: to avoid the best training possible (we want remove some columns, no fit the final model), in place of the lambda value comes up by the CV a smaller one will be used.

```{r}
# Fit the model with different lambda values
mod_pre <- cv.glmnet(mat_corr, train$y, alpha = 1, family = "binomial")

# Don't use the best lambda: 
lam_pre <- mod_pre$lambda.min/3
model_pre <- glmnet(mat_corr, train$y, alpha = 1, lambda = lam_pre, family = "binomial")

only_this <- as.numeric(model_pre$beta!=0)

cat("Initial number of features:",dim(mat_corr)[2] ,"\nFinal number of features:",sum(only_this))
```

# Find the most important features
The goal of the homework is to find the features of a dataset which affect a lot the results of the analysis. 
Now we're going to measure the most important features exploiting a "classical" method: random forest.

```{r}
numWorkers <- detectCores()  
cl <- makeCluster(numWorkers)
registerDoParallel(cl)

# Build 400-tree random forest 
rf <- foreach(ntree=rep(100, numWorkers),
              .combine=randomForest::combine,
              .multicombine=TRUE, 
              .packages='randomForest') %dopar% {
                randomForest(mat_corr[,only_this],
                             factor(train$y, levels = c(0,1)), 
                             ntree = ntree)
              }

stopCluster(cl)

# Take the 10 features most important
first_rf <- order(rf$importance,decreasing = T)[1:10]

print(paste("The train error using random forest is (with all the features):",round(sum(abs(as.numeric(rf$y)-as.numeric(rf$predicted)))/600,3)))

numWorkers <- detectCores()  
cl <- makeCluster(numWorkers)
registerDoParallel(cl)

# Build 400-tree random forest 
rf_new <- foreach(ntree=rep(100, numWorkers),
              .combine=randomForest::combine,
              .multicombine=TRUE, 
              .packages='randomForest') %dopar% {
                randomForest(mat_corr[,only_this][,first_rf],
                             factor(train$y, levels = c(0,1)), 
                             ntree = ntree)
              }

stopCluster(cl)

print(paste("The train error using random forest is (with only the firts ten features):",round(sum(abs(as.numeric(rf_new$y)-as.numeric(rf_new$predicted)))/600,3)))
```

Now is the moment of the special guess of the homework: LOCO!!!
LOCO (leave-one-covariate-out) is an other method to measure the importance of the features. In some sense, it expresses the extra error we'll find removing a feature from the dataset.

We want to compare these results with which come up by the random forest. We want to use LASSO as predictive model. First of all we can calculate the error on a validation set, using only the features selected previously.

```{r}
# Take only the survived features 
D_1_only_this <- D_1[,which(only_this==1,arr.ind=T)]
D_2_only_this <- D_2[,which(only_this==1,arr.ind=T)]

# Train the model with different lambda values
model_true_OT <- glmnet(D_1_only_this, lab_D_1, alpha = 1, lambda = 0, family = "binomial")

test_err <- sum(abs(as.numeric(predict(model_true_OT,D_2_only_this,type="class"))-lab_D_2))/length(lab_D_2)

cat("The error on the D_2 is:", test_err)
```
A good result, we could stop here though the homework hasn't started yet.

Now we're going to fit the model removing one feature at time and calculate the statistic $\theta$ provide by the referenced paper, using mean squared error loss. We don't use 0-1 loss because it's too harsh and we want to take in consideration the strength of the choices.
To estimate confidence interval we'll adopt nonparametric bootstrap. Moreover, we're estimating several 0.95 confidence intervals so it's necessary using some parameter correction such as the Bonferroni's: $\widetilde{\alpha}=0.0002$ and $z_{\widetilde{\alpha}/2} = -3.5$.

```{r}
# Fit all the remove the i-th column
mod_for <- list()
for (i in 1:model_pre$df){
  mod_for[[i]] <- glmnet(D_1_only_this[,-i], lab_D_1, alpha = 1, lambda = 0, family = "binomial")
}

# Function which we'll use to calculate the confidence intervall
fun_boot <- function(data, ind, feat, model_true, model_j){
  new_data <- data[ind,]
  true <- predict(model_true_OT, 
                  new_data,
                  type = "response")
  meno_j <- predict(model_j, 
                    new_data[,-feat], 
                    type = "response")
  return(sum((lab_D_2-as.numeric(meno_j))^2)-sum((lab_D_2-as.numeric(true))^2))
}  

# Estimation of the loss median 
b <- list()
theta <- rep(NA,model_pre$df)
conf_int <- matrix(NA,model_pre$df,2)
for (i in 1:model_pre$df){
  b[[i]] <- boot(D_2_only_this,
                 statistic = fun_boot,
                 R = 300,
                 feat = i,
                 model_true = model_pre,
                 model_j = mod_for[[i]],
                 ncpus = 4)
  s_d <- sd(b[[i]]$t)
  theta[i] <- mean(b[[i]]$t)
  conf_int[i,] <- c(theta[i]-s_d*3.5/sqrt(200),theta[i]+s_d*3.5/sqrt(200))
}

first_loco <- order(theta, decreasing=T)[1:10]
```

How are the importance scores distributed?

```{r}
values_rf <- scale(as.numeric(rf$importance),scale = F)
values_loco <- scale(as.numeric(theta), scale = F)

imp_rf <- data.frame(Method = factor(rep(c("RF", "LOCO"), each=model_pre$df)), 
                     imp = c(values_rf,values_loco))

ggplot(imp_rf, aes(x=imp, color=Method)) + 
  geom_histogram(fill = "white") +
  labs(x="Importance", 
       y="Frequence", 
       title = "Importance scores (random forest)") +
  geom_vline(aes(xintercept=values_rf[first_rf[10]]),
            color="blue", linetype="dashed", size=1) +
  geom_vline(aes(xintercept=values_loco[first_loco[10]]),
            color="red", linetype="dashed", size=1)

```

These are the histograms of the importance scores, scaled by the mean (we don't know if these values are comparable in this way, we're trying). The RF scores are closer each other, while the LOCO's ones are a bit more widespread. So, as showed by the dashed lines, the scores of the 10-th feature more important for the LOCO are more "evident" as results rather then the RF scores. 

The plot above is an overall visualization of the scores, it could be more interesting compare the 10-ths features more important for both the methods. 
As requested, we have to compare the best obtained by random forest with the best obtained with LOCO. 

```{r}
data = data.frame(first_rf,
                  first_loco,
                  rf_rf = values_rf[first_rf],
                  rf_theta = values_rf[first_loco],
                  theta_rf = values_loco[first_rf],
                  theta_theta = values_loco[first_loco])

tab = data %>%
  gt() %>%
   tab_spanner(
    label = "Best RF",
    columns = c(first_rf, rf_rf,theta_rf)
  ) %>% 
  tab_spanner(
    label = "Best LOCO",
    columns = c(first_loco,rf_theta,theta_theta))

tab
```

Only one column is common to both the methods, and generally the values are uncorrelated.

# Reduced to the bone

The paper gives us a way to select the best features, through an hypothesis test: in short we should remove all the columns if the relative confident interval contains the 0. It makes sense: it tells us that removing these features we don't account a big additional error.
Could be very interesting look to the confidence intervals.

```{r}
data <- round(data.frame(x = 1:model_pre$df,
                         y = theta[1:model_pre$df],
                         lower = conf_int[,1],
                         upper = conf_int[,2],
                         col = ifelse(conf_int[,1]>0,1,0)), 3)
ggplot(data, aes(x, y, color = col)) +        
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  labs(x="Features", 
       y="Theta", 
       title = "Theta confidence interval") +
  theme(legend.position='none')
  

```

Look to the above figure: the skier points represent the features which $\theta$'s confidence intervals doesn't contain the 0. Our last prove: let's try to fit LASSO model only with these columns, the most important.

```{r}
# Which conf. interval contains the 0
idx_conf <- which(conf_int[,1]>0, arr.ind = T)

D_1_final <- D_1_only_this[,idx_conf]
D_2_final <- D_2_only_this[,idx_conf]

model_true_final <- glmnet(D_1_final, lab_D_1, alpha = 1, lambda = 0, family = "binomial")

final_err <- sum(abs(as.numeric(predict(model_true_final,D_2_final,type = "class"))-lab_D_2))/length(lab_D_2)

cat("This is the final error on D_2:", final_err)

```

The less important features aren't irrelevant. No better results... Maybe we just wasted time...

